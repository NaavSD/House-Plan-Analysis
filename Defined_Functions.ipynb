{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd2d4bf",
   "metadata": {},
   "source": [
    "# List of Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23dfdb",
   "metadata": {},
   "source": [
    "## Pre-modelling Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6956aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a heatmap from a dataframe\n",
    "\n",
    "def heatmap(df):\n",
    "    plt.figure(figsize = (10,8))\n",
    "    sns.heatmap(df.corr().abs(), annot=True);\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0edcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates the Variance Inflation Factor of X_train variables\n",
    "\n",
    "def vif(X_train):\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    # defining an empty dataframe to capture the VIF scores\n",
    "    vif = pd.DataFrame()\n",
    "\n",
    "    # For each column,run a variance_inflaction_factor against all other columns to get a VIF Factor score\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X_train.values, i) for i in range(len(X_train.columns))]\n",
    "\n",
    "    # label the scores with their related columns\n",
    "    vif[\"features\"] = X_train.columns\n",
    "    \n",
    "    # print out the vif table and return\n",
    "    print(vif)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d334b510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8e623ee",
   "metadata": {},
   "source": [
    "## Post-Modelling Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66759b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qq(x_test, x_train, y_test, y_train):\n",
    "    # QQ plots are generally great tools for checking for normality.\n",
    "    import statsmodels.api as sm\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Calculating residuals\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    y_hat = lr.predict(x_test)\n",
    "    \n",
    "    residuals = y_test - y_hat\n",
    "    \n",
    "    \n",
    "    sm.qqplot(residuals, line = 'r');\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3bc189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sk_metrics(y, model):\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "    print(\"Metrics:\")\n",
    "    # R2\n",
    "    print(f\"R2: {r2_score(y, model):.3f}\")\n",
    "    # MAE\n",
    "    print(f\"Mean Absolute Error: {mean_absolute_error(y, model):.3f}\")\n",
    "    # MSE\n",
    "    print(f\"Mean Squared Error: {mean_squared_error(y, model):.3f}\")\n",
    "    # RMSE - just MSE but set squared=False\n",
    "    print(f\"Root Mean Squared Error: {mean_squared_error(y, model, squared=False):.3f}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54084cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sm_metrics(model, y, x):\n",
    "    # import associated tools\n",
    "    from statsmodels.tools.eval_measures import rmse, mse, meanabs\n",
    "    \n",
    "    # now generate predictions\n",
    "    ypred = model.predict(X)\n",
    "\n",
    "    # Print values\n",
    "    print('Metrics:')\n",
    "    # MAE\n",
    "    print(f\"Mean Absolute Error: {meanabs(y, ypred):.3f}\")\n",
    "    # MSE\n",
    "    print(f\"Mean Squared Error: {mse(y, ypred):.3f}\")\n",
    "    # RMSE\n",
    "    print(f\"Root Mean Squared Error: {rmse(y, ypred):.3f}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d77e2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model_metrics(x_test, x_train, y_test, y_train, cat_vars):\n",
    "    # Create scalers with non-cat vars\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(x_train.drop(cat_vars, axis=1))\n",
    "    x_train_scale = pd.DataFrame(ss.transform(x_train.drop(cat_vars, axis=1)))\n",
    "    x_test_scale = pd.DataFrame(ss.transform(x_test.drop(cat_vars, axis=1)))\n",
    "    \n",
    "    # One hot encoding cat vars\n",
    "    onehot = OneHotEncoder(sparse=False)\n",
    "    x_train_cat = pd.DataFrame(onehot.fit_transform(x_train[cat_vars]))\n",
    "    x_train_cat.columns = onehot.get_feature_names(cat_vars)\n",
    "    x_test_cat = pd.DataFrame(onehot.transform(x_test[cat_vars]))\n",
    "    x_test_cat.columns = onehot.get_feature_names(cat_vars)\n",
    "    \n",
    "    # Combine dummied cat vars with scaled vars\n",
    "    x_train_df = x_train_cat.join(x_train_scale)\n",
    "    x_test_df = x_test_cat.join(x_test_scale)\n",
    "    \n",
    "    # Run linear regression model for data\n",
    "    lr = LinearRegression()\n",
    "    model = lr.fit(x_train_df, y_train)\n",
    "    \n",
    "    print('Train Data')\n",
    "    sk_metrics(y_train, model.predict(x_train_df.values))\n",
    "    \n",
    "    print('Test Data')\n",
    "    sk_metrics(y_test, model.predict(x_test_df.values))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accb7e90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
